{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc309004",
   "metadata": {},
   "source": [
    "# Get frequency count for widows text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8e5f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk==3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee154f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e5136f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/oliviakasmin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/oliviakasmin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/oliviakasmin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import string\n",
    "punctuation = set(string.punctuation)\n",
    "stop_words_with_punct = stop_words.union(punctuation)\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "764002ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('df_grouped_NAID_sorted_title_with_file_cat.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8a5707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_widow = df[df['file_cat'].str.contains('widow', case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622faab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25517, 22)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_widow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c85dca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 25517\n",
      "Rows with transcriptionText: 6756\n",
      "Percentage: 26.48%\n"
     ]
    }
   ],
   "source": [
    "rows_with_transcription = ((df['transcriptionText'].notna()) & (df_widow['transcriptionText'] != '')).sum()\n",
    "total_rows = len(df_widow)\n",
    "percentage = (rows_with_transcription / total_rows) * 100\n",
    "\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Rows with transcriptionText: {rows_with_transcription}\")\n",
    "print(f\"Percentage: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c01381",
   "metadata": {},
   "source": [
    "### Clean ocrText\n",
    "\n",
    "1. split on \"||\"\n",
    "2. tokenize words\n",
    "3. filter to remove stop words\n",
    "4. lemmatize words to get more consistent results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df_widow.iloc[0]['ocrText']\n",
    "sample_str = sample.replace(\"||\", \" \").replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53531d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First tokenize the text into words\n",
    "words = word_tokenize(sample_str)\n",
    "\n",
    "# Then filter out stop words\n",
    "meaningful_words = [word for word in words if word.casefold() not in stop_words_with_punct]\n",
    "\n",
    "# Then lemmatize\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in meaningful_words]\n",
    "\n",
    "# Then get frequency distribution\n",
    "frequency_distribution = FreqDist(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "61d08b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'Livingston': 94, 'York': 34, 'Henry': 33, 'Jane': 31, 'New': 30, 'said': 28, 'day': 25, '1840': 19, 'Major': 18, 'County': 18, ...})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b8c7c3",
   "metadata": {},
   "source": [
    "Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4a5b62b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York; Smith Thompson; Henry Livingston; day March; Commissioner\n",
      "Pensions; United Colonies; duly sworn; hand seal; Know men; Notary\n",
      "Public; Livingston decd; Livingston Junior; pension allowed; State\n",
      "New; constitute appoint; men present; Henry Gilbert; Robert Gilbert;\n",
      "April 1840; Edwards Esq\n"
     ]
    }
   ],
   "source": [
    "new_text = nltk.Text(lemmatized_words)\n",
    "new_text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eb9229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ocr_text(ocrText):\n",
    "    str = ocrText.replace(\"||\", \" \").replace(\"\\n\", \" \")\n",
    "   \n",
    "    # First tokenize the text into words\n",
    "    words = word_tokenize(str)\n",
    "\n",
    "    # Then filter out stop words\n",
    "    meaningful_words = [word for word in words if word.casefold() not in stop_words_with_punct]\n",
    "\n",
    "    # Then lemmatize\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in meaningful_words]\n",
    "\n",
    "    # Then get frequency distribution\n",
    "    frequency_distribution = FreqDist(lemmatized_words)\n",
    "\n",
    "\n",
    "    # text = nltk.Text(lemmatized_words)\n",
    "    # collocations = text.collocations()\n",
    "\n",
    "    # print(collocations)\n",
    "\n",
    "    return frequency_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1c43c8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = analyze_ocr_text(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb92e14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FreqDist({'Livingston': 94, 'York': 34, 'Henry': 33, 'Jane': 31, 'New': 30, 'said': 28, 'day': 25, '1840': 19, 'Major': 18, 'County': 18, ...})]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c58996e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ocr_text(ocrText):\n",
    "    # Clean the text\n",
    "    text_str = ocrText.replace(\"||\", \" \").replace(\"\\n\", \" \")\n",
    "   \n",
    "    # First tokenize the text into words\n",
    "    words = word_tokenize(text_str)\n",
    "\n",
    "    # Then filter out stop words and punctuation\n",
    "    meaningful_words = [word for word in words if word.casefold() not in stop_words_with_punct]\n",
    "\n",
    "    # Then lemmatize\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in meaningful_words]\n",
    "\n",
    "    # Get frequency distribution\n",
    "    frequency_distribution = FreqDist(lemmatized_words)\n",
    "\n",
    "    # Get bigram collocations\n",
    "    # bigram_finder = BigramCollocationFinder.from_words(lemmatized_words)\n",
    "    # bigram_collocations = bigram_finder.nbest(BigramAssocMeasures.raw_freq, 20)\n",
    "    \n",
    "    # Get trigram collocations\n",
    "    # trigram_finder = TrigramCollocationFinder.from_words(lemmatized_words)\n",
    "    # trigram_collocations = trigram_finder.nbest(TrigramAssocMeasures.raw_freq, 20)\n",
    "    \n",
    "    freq_dict = dict(frequency_distribution)\n",
    "    sorted_freq_dict = dict(sorted(freq_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    return {\n",
    "        'frequency_distribution': frequency_distribution,\n",
    "        'frequency_dict': sorted_freq_dict, \n",
    "        # 'bigram_collocations': bigram_collocations,\n",
    "        # 'trigram_collocations': trigram_collocations,\n",
    "        # 'lemmatized_words': lemmatized_words\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e6e48f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = analyze_ocr_text(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cacb5f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Edwin', 'G.', 'Livingston'),\n",
       " ('Henry', 'Livingston', 'Junior'),\n",
       " ('Jane', 'M.', 'Livingston'),\n",
       " ('State', 'New', 'York'),\n",
       " ('Charles', 'P.', 'Livingston'),\n",
       " ('Helen', 'P.', 'Livingston'),\n",
       " ('Henry', 'Gilbert', 'Livingston'),\n",
       " ('Livingston', 'Helen', 'P.'),\n",
       " ('Major', 'Henry', 'Livingston'),\n",
       " ('Susan', 'C.', 'Livingston'),\n",
       " ('widow', 'Henry', 'Livingston'),\n",
       " ('Army', 'United', 'Colonies'),\n",
       " ('County', 'New', 'York'),\n",
       " ('Given', 'hand', 'seal'),\n",
       " ('Henry', 'Livingston', 'decd'),\n",
       " ('James', 'L.', 'Edwards'),\n",
       " ('Jane', 'Livingston', 'widow'),\n",
       " ('Know', 'men', 'present'),\n",
       " ('Livingston', 'New', 'York'),\n",
       " ('Livingston', 'deceased', 'widow')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['trigram_collocations']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a6e059",
   "metadata": {},
   "source": [
    "## Process all widows df to get overall frequency distrubtion, bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5bd5775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all ocrText from df_widow\n",
    "all_text = \" \".join(df_widow['ocrText'].dropna().astype(str))\n",
    "\n",
    "# Clean the combined text\n",
    "all_text_cleaned = all_text.replace(\"||\", \" \").replace(\"\\n\", \" \").lower()\n",
    "\n",
    "# Tokenize the combined text\n",
    "all_words = word_tokenize(all_text_cleaned)\n",
    "\n",
    "# Filter out stop words and punctuation\n",
    "all_meaningful_words = [\n",
    "    word for word in all_words \n",
    "    if word.casefold() not in stop_words_with_punct\n",
    "]\n",
    "\n",
    "# Lemmatize\n",
    "all_lemmatized_words = [lemmatizer.lemmatize(word) for word in all_meaningful_words]\n",
    "\n",
    "# Get frequency distribution for the combined text\n",
    "combined_freq_dist = FreqDist(all_lemmatized_words)\n",
    "\n",
    "# Get bigram collocations from combined text\n",
    "bigram_finder = BigramCollocationFinder.from_words(all_lemmatized_words)\n",
    "bigram_collocations = bigram_finder.nbest(BigramAssocMeasures.raw_freq, 20)\n",
    "\n",
    "# Get trigram collocations from combined text\n",
    "trigram_finder = TrigramCollocationFinder.from_words(all_lemmatized_words)\n",
    "trigram_collocations = trigram_finder.nbest(TrigramAssocMeasures.raw_freq, 20)\n",
    "\n",
    "# Results\n",
    "# print(\"Top 20 most common words (combined corpus):\")\n",
    "# print(combined_freq_dist.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e9c9debe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most common words (combined corpus):\n",
      "[('said', 1361693), ('county', 1154822), ('state', 970883), ('day', 852895), ('year', 710170), ('pension', 706477), ('court', 551557), ('service', 496139), ('act', 472814), ('widow', 425544), ('war', 390089), ('john', 383063), ('time', 370861), ('one', 359410), ('certify', 348172), ('new', 346566), ('--', 327790), ('sworn', 325878), ('march', 319647), ('served', 291514), ('clerk', 290833), ('justice', 290665), ('july', 272634), ('may', 270945), ('month', 267304), ('declaration', 266618), ('record', 265027), ('name', 264974), ('peace', 262056), ('certificate', 261764), ('office', 256602), ('made', 254589), ('revolutionary', 253770), ('1', 251145), ('aforesaid', 248774), ('subscribed', 239725), ('┃', 236500), ('june', 233587), ('york', 222730), ('company', 217343)]\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "print(\"Top 20 most common words (combined corpus):\")\n",
    "print(combined_freq_dist.most_common(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b79d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop 20 bigram collocations (combined corpus):\")\n",
    "for word1, word2 in bigram_collocations:\n",
    "    print(f\"  {word1} {word2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "02080b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 trigram collocations (combined corpus):\n",
      "  State New York\n",
      "  duly sworn according\n",
      "  hereunto set hand\n",
      "  whereof hereunto set\n",
      "  sworn according law\n",
      "  order obtain benefit\n",
      "  testimony whereof hereunto\n",
      "  first duly sworn\n",
      "  Certificate Pension issued\n",
      "  according law doth\n",
      "  oath make following\n",
      "  doth oath make\n",
      "  4th day March\n",
      "  per annum commence\n",
      "  thousand eight hundred\n",
      "  set hand affixed\n",
      "  subscribed day year\n",
      "  Cents per annum\n",
      "  law doth oath\n",
      "  Court Common Pleas\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop 20 trigram collocations (combined corpus):\")\n",
    "for word1, word2, word3 in trigram_collocations:\n",
    "    print(f\"  {word1} {word2} {word3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5be304c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'widow_full_text_analysis_results.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Prepare data for JSON serialization\n",
    "results = {\n",
    "    'frequency_distribution': combined_freq_dist.most_common(),  # List of (word, count) tuples\n",
    "    'bigram_collocations': [list(bigram) for bigram in bigram_collocations],\n",
    "    'trigram_collocations': [list(trigram) for trigram in trigram_collocations]\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open('widow_full_text_analysis_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to 'widow_full_text_analysis_results.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26acfd52",
   "metadata": {},
   "source": [
    "## Process all rows that have transcriptionText so can compare results from just widows to get frequency distrubtion, bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2abefd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all ocrText from df_widow\n",
    "all_cats_text = \" \".join(df['transcriptionText'].dropna().astype(str))\n",
    "\n",
    "# Clean the combined text\n",
    "all_cats_text_cleaned = all_cats_text.replace(\"||\", \" \").replace(\"\\n\", \" \").lower()\n",
    "\n",
    "# Tokenize the combined text\n",
    "all_cats_words = word_tokenize(all_cats_text_cleaned)\n",
    "\n",
    "# Filter out stop words and punctuation\n",
    "all_cats_meaningful_words = [\n",
    "    word for word in all_cats_words \n",
    "    if word.casefold() not in stop_words_with_punct\n",
    "]\n",
    "\n",
    "# Lemmatize\n",
    "all_cats_lemmatized_words = [lemmatizer.lemmatize(word) for word in all_cats_meaningful_words]\n",
    "\n",
    "# Get frequency distribution for the combined text\n",
    "combined_cats_freq_dist = FreqDist(all_cats_lemmatized_words)\n",
    "\n",
    "# Get bigram collocations from combined text\n",
    "bigram_cats_finder = BigramCollocationFinder.from_words(all_cats_lemmatized_words)\n",
    "bigram_cats_collocations = bigram_cats_finder.nbest(BigramAssocMeasures.raw_freq, 20)\n",
    "\n",
    "# Get trigram collocations from combined text\n",
    "trigram_cats_finder = TrigramCollocationFinder.from_words(all_cats_lemmatized_words)\n",
    "trigram_cats_collocations = trigram_cats_finder.nbest(TrigramAssocMeasures.raw_freq, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d6d55b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('county', 658938), ('said', 641565), ('state', 608343), ('pension', 458630), ('day', 438489), ('service', 421794), ('year', 417024), ('court', 369497), ('war', 300698), ('illegible', 276604), ('--', 251536), ('john', 251028), ('act', 243030), ('served', 240179), ('time', 220085), ('one', 209059), ('month', 205624), ('revolutionary', 199902), ('sworn', 195004), ('name', 191195), ('new', 189591), ('certify', 185744), ('declaration', 177471), ('office', 176320), ('record', 172710), ('march', 171064), ('company', 164356), (\"'s\", 160403), ('clerk', 159214), ('certificate', 156325), ('claim', 156165), ('june', 155396), ('may', 155237), ('soldier', 151009), ('widow', 149523), ('justice', 147878), ('aforesaid', 146794), ('seal', 141979), ('made', 140151), ('``', 135929)]\n"
     ]
    }
   ],
   "source": [
    "print(combined_cats_freq_dist.most_common(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9d5371a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('--', '--'), ('revolutionary', 'war'), ('said', 'county'), ('justice', 'peace'), ('new', 'york'), ('united', 'state'), ('hereby', 'certify'), ('personally', 'appeared'), ('said', 'court'), ('state', 'new'), ('sworn', 'subscribed'), ('duly', 'sworn'), ('county', 'state'), ('act', 'congress'), ('testimony', 'whereof'), ('whereof', 'hereunto'), ('set', 'hand'), ('day', 'march'), ('according', 'law'), ('day', 'year')]\n"
     ]
    }
   ],
   "source": [
    "print(bigram_cats_collocations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "56f4f339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('--', '--', '--'), ('testimony', 'whereof', 'hereunto'), ('state', 'new', 'york'), ('sworn', 'according', 'law'), ('hereunto', 'set', 'hand'), ('whereof', 'hereunto', 'set'), ('duly', 'sworn', 'according'), ('court', 'common', 'plea'), ('declaration', 'order', 'obtain'), ('sworn', 'subscribed', 'day'), ('subscribed', 'day', 'year'), ('according', 'law', 'doth'), ('order', 'obtain', 'benefit'), ('first', 'duly', 'sworn'), ('justice', 'peace', 'said'), ('thousand', 'eight', 'hundred'), ('law', 'doth', 'oath'), ('one', 'thousand', 'eight'), ('peace', 'said', 'county'), ('service', 'united', 'state')]\n"
     ]
    }
   ],
   "source": [
    "print(trigram_cats_collocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "819ab5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 20 words from each frequency distribution\n",
    "top_20_freq = set([word for word, count in combined_freq_dist.most_common(200)])\n",
    "top_20_cats = set([word for word, count in combined_cats_freq_dist.most_common(200)])\n",
    "\n",
    "# Find unique words in top 20 of combined_freq_dist (not in top 20 of combined_cats_freq_dist)\n",
    "unique_in_freq = top_20_freq - top_20_cats\n",
    "\n",
    "# Find unique words in top 20 of combined_cats_freq_dist (not in top 20 of combined_freq_dist)\n",
    "unique_in_cats = top_20_cats - top_20_freq\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "702950b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in top 20 of combined_freq_dist but NOT in top 100 of combined_cats_freq_dist:\n",
      "  1838: 114829\n",
      "  1843: 79295\n",
      "  a.d.: 74193\n",
      "  annexed: 74625\n",
      "  annum: 100018\n",
      "  benefit: 80057\n",
      "  c: 74065\n",
      "  connecticut: 88931\n",
      "  deceased: 73932\n",
      "  elizabeth: 80697\n",
      "  father: 79345\n",
      "  full: 75075\n",
      "  known: 77815\n",
      "  late: 84758\n",
      "  man: 78344\n",
      "  massachusetts: 108438\n",
      "  mr: 120530\n",
      "  part: 76252\n",
      "  rate: 99504\n",
      "  six: 76078\n",
      "  smith: 79034\n",
      "  son: 104171\n",
      "  thousand: 77869\n",
      "  twenty: 85551\n",
      "  w: 75963\n",
      "  ⎬: 93569\n",
      "  ┃: 236500\n",
      "  ●: 74356\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "print(\"Words in top 20 of combined_freq_dist but NOT in top 100 of combined_cats_freq_dist:\")\n",
    "for word in sorted(unique_in_freq):\n",
    "    count = combined_freq_dist[word]\n",
    "    print(f\"  {word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0b8e81c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words in top 20 of combined_cats_freq_dist but NOT in top 100 of combined_freq_dist:\n",
      "  Carolina: 78372\n",
      "  Col.: 51666\n",
      "  Colonel: 58035\n",
      "  District: 48412\n",
      "  General: 48735\n",
      "  Henry: 43931\n",
      "  L.: 44364\n",
      "  Mrs.: 45556\n",
      "  North: 49761\n",
      "  Pennsylvania: 45392\n",
      "  Rev: 45492\n",
      "  appears: 49837\n",
      "  blank: 48161\n",
      "  called: 44073\n",
      "  county: 94499\n",
      "  court: 65679\n",
      "  family: 44426\n",
      "  given: 49537\n",
      "  illegible: 262961\n",
      "  letter: 58848\n",
      "  living: 45816\n",
      "  must: 62918\n",
      "  opinion: 50963\n",
      "  period: 44600\n",
      "  regiment: 46142\n",
      "  signed: 64555\n",
      "  statement: 51350\n",
      "  term: 47107\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nWords in top 20 of combined_cats_freq_dist but NOT in top 100 of combined_freq_dist:\")\n",
    "for word in sorted(unique_in_cats):\n",
    "    count = combined_cats_freq_dist[word]\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6bd1f7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "  Unique in combined_freq_dist: 28 words\n",
      "  Unique in combined_cats_freq_dist: 28 words\n",
      "  Common words: 172 words\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Unique in combined_freq_dist: {len(unique_in_freq)} words\")\n",
    "print(f\"  Unique in combined_cats_freq_dist: {len(unique_in_cats)} words\")\n",
    "print(f\"  Common words: {len(top_20_freq & top_20_cats)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5d958f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'cats_text_analysis_results.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Prepare data for JSON serialization\n",
    "cats_results = {\n",
    "    'frequency_distribution': combined_cats_freq_dist.most_common(),  # List of (word, count) tuples\n",
    "    'bigram_collocations': [list(bigram) for bigram in bigram_cats_collocations],  # Convert tuples to lists\n",
    "    'trigram_collocations': [list(trigram) for trigram in trigram_cats_collocations]  # Convert tuples to lists\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open('cats_text_analysis_results.json', 'w') as f:\n",
    "    json.dump(cats_results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to 'cats_text_analysis_results.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b8c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pension-files",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
